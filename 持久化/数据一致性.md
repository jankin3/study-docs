# 数据一致性

读多写少的应用场景，我们经常使用缓存来进行优化. 缓存技术是一种提高系统读性能的常见技术。

缓存的设计一般有以下几种：

1. cache aside
2. read/write through
3. Write Back



### cache aside

一般使用最多的是这种模式，具体实现如下：

- **失效**：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。

- **命中**：应用程序从cache中取数据，取到后返回。

- **更新**：先把数据存到数据库中，成功后，再让缓存失效。



然而又有了下面几个问题？

1. 先操作数据库还是先操作缓存？

如果是先操作缓存再操作数据库：两个并发操作，一个是更新操作，另一个是查询操作，更新操作删除缓存后，查询操作没有命中缓存，先把老数据读出来后放到缓存中，然后更新操作更新了数据库。于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了。



2. 把数据存到数据库中，成功后，是删除缓存还是更新缓存？

![图片](https://mmbiz.qpic.cn/mmbiz_png/RXvHpViaz3EoXIeA7IYD5r2u1MJ05slAFrlsyFMmGndYAfhyxhXdOg71Bibib5r2uVzT20qdJEXQticfZzPVcMLdzQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

如上图的执行过程：

（1）`写请求1`更新数据库，将 age 字段更新为18；

（2）`写请求2`更新数据库，将 age 字段更新为20；

（3）`写请求2`更新缓存，缓存 age 设置为20；

（4）`写请求1`更新缓存，缓存 age 设置为18；

执行完预期结果是数据库 age 为20，缓存 age 为20，结果缓存 age为18，这就造成了缓存数据不是最新的，出现了脏数据。

然而删除缓存也存在的一些问题：

![图片](https://mmbiz.qpic.cn/mmbiz_png/RXvHpViaz3EoXIeA7IYD5r2u1MJ05slAFmh4AT070YGbC9w9OstZkvyL5eDddFe2lstDIofPxTmHJNel6wGaH3w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

如上图的执行过程：

（1）`读请求`先查询缓存，缓存未击中，查询数据库返回数据；

（2）`写请求`更新数据库，删除缓存；

（3）`读请求`回写缓存；

整个流程操作下来发现`数据库age为20`，`缓存age为18`，即数据库与缓存不一致，导致应用程序从缓存中读到的数据都为旧数据。

但我们仔细想一下，上述问题发生的概率其实非常低，因为通常数据库更新操作比内存操作耗时多出几个数量级，上图中最后一步回写缓存（set age 18）速度非常快，通常会在更新数据库之前完成。

如果这种极端场景出现了怎么办？我们得想一个兜底的办法：`缓存数据设置过期时间`。通常在系统中是可以允许少量的数据短时间不一致的场景出现。



### read/ write throght

cache aside中应用程序总是要操作`缓存`和`数据库`两个数据存储。如果我们能提供一个服务来包装这两个系统，而对应用程序保持透明那样就会简单很多。而read/ write throght就是为此诞生。

##### Read Through

Read Through 套路就是在查询操作中更新缓存，也就是说，当缓存失效的时候（过期或LRU换出），Cache Aside是由调用方负责把数据加载入缓存，而Read Through则用缓存服务自己来加载，从而对应用方是透明的。

##### Write Through

Write Through 套路和Read Through相仿，不过是在更新数据时发生。当有数据更新的时候，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后再由Cache自己更新数据库（这是一个同步操作）



### Write Back

Write Back套路，一句说就是，在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是让数据的I/O操作飞快无比（因为直接操作内存嘛 ），因为异步，write backg还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。

而同时存在问题就是数据不是强一致性的。





参考：

https://coolshell.cn/articles/17416.html#comments

https://mp.weixin.qq.com/s/ODVdqLL81O-_dv_cmb9HkQ

